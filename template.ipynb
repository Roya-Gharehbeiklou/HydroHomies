{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HydroHomies Plots\n",
    "In this notebook, the plots, figures and also some explanations or details about each of them are being presented.  \n",
    "\n",
    "To clarify plots, please follow this order:\n",
    "- Title for each plot is mandatory\n",
    "- Analysis must be written \n",
    "- legends are manedatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.transform import dodge, factor_cmap\n",
    "from bokeh.models import ColumnDataSource, FactorRange, Whisker\n",
    "import panel as pn\n",
    "from scipy.stats import sem\n",
    "\n",
    "output_notebook()\n",
    "pn.extension()\n",
    "\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as stream:\n",
    "    config = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "code1"
    ]
   },
   "source": [
    "### Cleaning (Digit Span Raw Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_digit_span(raw_df):\n",
    "    # Select the sequence length data from the raw data and create a dataframe\n",
    "    seq_length_df = raw_df[raw_df[1].astype(str).str.match(r'\\d+')]\n",
    "\n",
    "    # Get the value of the longest sequence remembered\n",
    "    longest = seq_length_df[2]\n",
    "    longest = longest.tolist()\n",
    "\n",
    "    # Get the number of errors made\n",
    "    error_number = seq_length_df[3]\n",
    "    error_number = error_number.tolist()\n",
    "\n",
    "    # Select the rows with the click stimulus data\n",
    "    click_stim_df = raw_df[raw_df[1]=='clickedStim']\n",
    "    click_stim_df.size\n",
    "\n",
    "    # Calculate the number of clicks made by the participant\n",
    "    clicks_observed = click_stim_df.count(axis=1) - 2 \n",
    "    clicks_observed = clicks_observed.tolist()\n",
    "\n",
    "    # Calculate the number of clicks that the participant should have made\n",
    "    clicks_expected =  pd.to_numeric(longest) + 1\n",
    "    clicks_expected = clicks_expected.tolist()\n",
    "\n",
    "    # Create a new dataframe with all the values calculated above\n",
    "    clean_data = pd.DataFrame(data ={'seq length':longest,\n",
    "                        'errors': error_number,\n",
    "                        'clicks expected': clicks_expected,\n",
    "                        'clicks observed':clicks_observed})\n",
    "\n",
    "    # Return the new dataframe\n",
    "    return clean_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Integration For Each Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_df(config_dict):\n",
    "    data_dict = {}\n",
    "\n",
    "    # read the files \n",
    "    for test, file in config_dict.items():\n",
    "        df_dict = pd.read_excel(file, sheet_name=None, header=None)\n",
    "\n",
    "        for session, df in df_dict.items():\n",
    "\n",
    "            # extracting the participant name and type name\n",
    "            participant = test.split('_')[-1]\n",
    "            test_name = test.split('_')[0]\n",
    "\n",
    "            #extracting repeat number and making its column except for personal\n",
    "            try:\n",
    "                type, repeat = session.split('_')\n",
    "                df.insert(0, 'repeat', repeat)\n",
    "\n",
    "            except ValueError:\n",
    "                type = session\n",
    "\n",
    "\n",
    "            # Running function to clean digit span data\n",
    "            if test_name == 'digit':\n",
    "                df = clean_digit_span(df.iloc[3:])\n",
    "                df.insert(0, 'repeat', repeat)\n",
    "            \n",
    "            # verbal fluency test contains header\n",
    "            elif test_name =='verbal':\n",
    "                df = df.iloc[1:]\n",
    "\n",
    "            # inserting the type and participant columns\n",
    "            df.insert(0, 'type', type)\n",
    "            df.insert(0, 'participant', participant)\n",
    " \n",
    "            # concatenating data frames of each test\n",
    "            if test_name not in data_dict:\n",
    "                data_dict[test_name] = df\n",
    "            else:\n",
    "                data_dict[test_name] = pd.concat([data_dict[test_name], df])\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "data_dict = create_merged_df(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function\n",
    "df_dict = create_merged_df(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flanker Test Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Flanker dataframe\n",
    "def create_flanker_dataframe():\n",
    "    flanker_df = data_dict[\"flanker\"]\n",
    "    flanker_df.rename(columns={0: \"pattern\", 1: \"expression\", 2: \"correctness\", 3: \"response-time\"}, inplace=True)\n",
    "    flanker_df[\"correctness\"] = flanker_df[\"correctness\"].replace(1, \"correct\")\n",
    "    flanker_df[\"correctness\"] = flanker_df[\"correctness\"].replace(2, \"incorrect\")\n",
    "    flanker_df[\"correctness\"] = flanker_df[\"correctness\"].replace(3, \"not-answer\")\n",
    "    return flanker_df\n",
    "\n",
    "flanker_df = create_flanker_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(data, title, x_label=\"\", y_label=\"\", palette=[\"salmon\", \"skyblue\"], factors=[\"dehydration\", \"control\"]):\n",
    "    index_cmap = factor_cmap('x', palette=palette, factors=factors, start=1, end=2)\n",
    "    x = list(data.index.values)\n",
    "    data_map = {\n",
    "        'x': x,\n",
    "        'counts': data.tolist()\n",
    "        }\n",
    "\n",
    "    source = ColumnDataSource(data=data_map)\n",
    "    p = figure(x_range=FactorRange(*x), y_range=(0, 100), height=400, title=title,\n",
    "               toolbar_location=None, tools=\"\", x_axis_label=x_label, y_axis_label=y_label)\n",
    "\n",
    "    p.vbar(x='x', top='counts', width=0.9, source=source, fill_color=index_cmap)\n",
    "\n",
    "    p.y_range.start = 0\n",
    "    p.x_range.range_padding = 0.1\n",
    "    p.xaxis.major_label_orientation = 1\n",
    "    p.xgrid.grid_line_color = None\n",
    "    return p\n",
    "\n",
    "def flanker_calculate_counts(flanker_df, answer_type=\"correct\"):\n",
    "    flanker_df = flanker_df[flanker_df[\"correctness\"] == answer_type]\n",
    "    flanker_df = flanker_df.groupby([\"participant\", \"type\", \"repeat\"])[\"correctness\"].count().reset_index()\n",
    "    data = flanker_df.groupby(by=[\"participant\", \"type\"])[\"correctness\"].mean()\n",
    "    return data\n",
    "\n",
    "def flanker_plot_count(answer_type=\"correct\"):  # Roya\n",
    "    flanker_df = create_flanker_dataframe()\n",
    "    data = flanker_calculate_counts(flanker_df, answer_type)\n",
    "    return show_plot(data, f\"Average of {answer_type} answers\", \"participant/session\", \"count\" )\n",
    "\n",
    "answer_types =['correct','incorrect']\n",
    "inter_plot = pn.interact(flanker_plot_count, answer_type = answer_types)\n",
    "inter_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage(flanker_df, answer_type=\"correct\"):\n",
    "    df_all = flanker_df.groupby([\"participant\", \"type\", \"repeat\"]).agg(count=(\"correctness\", \"count\"))\n",
    "    flanker_df = flanker_df[flanker_df[\"correctness\"] == answer_type]\n",
    "    df_correct = flanker_df.groupby([\"participant\", \"type\", \"repeat\"]).agg(count=(\"correctness\", \"count\"))\n",
    "\n",
    "    flanker_df = round(df_correct[\"count\"] * 100 / df_all[\"count\"], 2).rename(\"correctness\").reset_index().fillna(0)\n",
    "    data = flanker_df.groupby(by=[\"participant\", \"type\"])[\n",
    "        \"correctness\"].mean()\n",
    "    return data\n",
    "    \n",
    "def flanker_plot_percentage(answer_type=\"correct\"):  # Roya\n",
    "    flanker_df = create_flanker_dataframe()\n",
    "    data = calculate_percentage(flanker_df, answer_type)\n",
    "    return show_plot(data, f\"Percentage of {answer_type} answers\", \"participant/session\", \"Percentage\" )\n",
    "\n",
    "\n",
    "\n",
    "answer_types =['correct','incorrect']\n",
    "inter_plot = pn.interact(flanker_plot_percentage, answer_type = answer_types)\n",
    "inter_plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_standard_error(flanker_df, answer_type=\"correct\"):\n",
    "    flanker_df = flanker_df[flanker_df[\"correctness\"] == answer_type]\n",
    "    flanker_df = flanker_df.groupby([\"participant\", \"type\", \"repeat\"])[\"correctness\"].count().reset_index()\n",
    "    df_mean = flanker_df.groupby(by=[\"participant\", \"type\"]).agg(mean=(\"correctness\", \"mean\"))\n",
    "    df_se = flanker_df.groupby(by=[\"participant\", \"type\"]).agg(se=(\"correctness\", \"sem\"))\n",
    "    upper = df_mean[\"mean\"] + 1.96 * df_se[\"se\"]\n",
    "    lower = df_mean[\"mean\"] - 1.96 * df_se[\"se\"]\n",
    "    data = pd.concat([upper.rename(\"upper\"), lower.rename(\"lower\")], axis=1)\n",
    "    return data\n",
    "\n",
    "def plot_standard_error(plot, data):\n",
    "    x = list(data.index.values)\n",
    "    data_map = {\n",
    "        'x': x,\n",
    "        'upper': data[\"upper\"].tolist(),\n",
    "        'lower': data[\"lower\"].tolist()\n",
    "\n",
    "        }\n",
    "    source = ColumnDataSource(data=data_map)\n",
    "\n",
    "    w = Whisker(source=source, base=\"x\", upper=\"upper\", lower=\"lower\", \n",
    "            line_color='purple', level=\"overlay\")\n",
    "    w.upper_head.line_color = 'purple'\n",
    "    w.lower_head.line_color = 'purple'\n",
    "    w.upper_head.size = w.lower_head.size = 20\n",
    "    plot.add_layout(w)\n",
    "    return plot\n",
    "\n",
    "def flanker_plot_error(answer_type=\"correct\"):  # Roya\n",
    "    flanker_df = create_flanker_dataframe()\n",
    "    data = flanker_calculate_counts(flanker_df, answer_type)\n",
    "    data_se = calculate_standard_error(flanker_df, answer_type)\n",
    "    p = show_plot(data, f\"Standard Error of {answer_type} answers\", \"participant/session\", \"SE\" )\n",
    "    p = plot_standard_error(p, data_se)\n",
    "    return p\n",
    "\n",
    "\n",
    "answer_types =['correct','incorrect']\n",
    "inter_plot = pn.interact(flanker_plot_error, answer_type = answer_types)\n",
    "inter_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stroop Test  Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stroop_test(): # Mahdiye\n",
    "    total_dict = create_merged_df(config)\n",
    "    stroop_df = total_dict['stroop']\n",
    "    stroop_df.drop(stroop_df.columns[[3,7]], axis=1, inplace=True)\n",
    "    stroop_df = stroop_df.set_axis(['participant', 'type','repeat','word name','word color',\n",
    "                                    'name_color match','pressed _key','status','reaction_time'], axis=1)\n",
    "    stroop_df['type&repeat'] = stroop_df['type']+stroop_df['repeat']\n",
    "    return stroop_df\n",
    "\n",
    "stroop_df = stroop_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, Whisker\n",
    "import panel as pn\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_stroop_bar_plot(participant='blue'):\n",
    "    df = stroop_df[stroop_df['participant']==participant]\n",
    "    \n",
    "    dff= df.groupby('type&repeat').min().reset_index()\n",
    "    p = figure(x_range=dff['type&repeat'], height=350, toolbar_location=None, \n",
    "               title=f'Stroop Test {participant}', y_axis_label=\"Reaction time(milliseconds)\")\n",
    "    p.vbar(x=dff['type&repeat'], bottom=0,top=dff['reaction_time'], width=0.5, line_color='white', color=participant)\n",
    "    return p\n",
    "\n",
    "#interactive plots\n",
    "participants_color =['blue','red','orange','green','pink']\n",
    "inter_plot = pn.interact(individual_stroop_bar_plot, participant = participants_color)\n",
    "inter_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_stroop_box_plot(participant):\n",
    "    \n",
    "    df = stroop_df[stroop_df['participant']==participant]\n",
    "    kinds = df['type&repeat'].unique()\n",
    "    \n",
    "    # compute quantiles\n",
    "    qs = df.groupby('type&repeat').reaction_time.quantile([0.25, 0.5, 0.75])\n",
    "    qs = qs.unstack().reset_index()\n",
    "    qs.columns = ['type&repeat', \"q1\", \"q2\", \"q3\"]\n",
    "    df = pd.merge(df, qs, on='type&repeat', how=\"left\")\n",
    "\n",
    "    # compute IQR outlier bounds\n",
    "    iqr = df.q3 - df.q1\n",
    "    df[\"upper\"] = df.q3 + 1.5*iqr\n",
    "    df[\"lower\"] = df.q1 - 1.5*iqr\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    p = figure(x_range=kinds,y_range=[0,stroop_df['reaction_time'].max() * 1.3],tools=\"\", toolbar_location=None,\n",
    "                title=\"box plot of stroop test \"+participant,\n",
    "               background_fill_color=\"#eaefef\", y_axis_label=\"Reaction time(milliseconds)\")\n",
    "\n",
    "\n",
    "    # outlier range\n",
    "    whisker = Whisker(base='type&repeat', upper=\"upper\", lower=\"lower\", source=source)\n",
    "    whisker.upper_head.size = whisker.lower_head.size = 20\n",
    "    p.add_layout(whisker)\n",
    "\n",
    "    # quantile boxes\n",
    "    p.vbar('type&repeat', 0.5, \"q2\", \"q3\", color = participant,bottom=0, source=source, line_color=\"black\")\n",
    "    p.vbar('type&repeat', 0.5, \"q1\", \"q2\", color=participant, bottom=0, source=source, line_color=\"black\")\n",
    "    \n",
    "    # outliers\n",
    "    outliers = df[~df.reaction_time.between(df.lower, df.upper)]\n",
    "    p.scatter('type&repeat', 'reaction_time', source=outliers, size=6, color=\"black\", alpha=0.5)\n",
    "\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.major_label_text_font_size=\"14px\"\n",
    "    p.axis.axis_label_text_font_size=\"12px\"\n",
    "\n",
    "    return p\n",
    "    \n",
    "#interactive plots\n",
    "participants_color =['blue','red','orange','green','pink']\n",
    "inter_plot = pn.interact(individual_stroop_box_plot, participant = participants_color)\n",
    "inter_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Signal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_meanings = {'Column':[0,1,2,3,4,5,6,7],\n",
    "                   'Meaning':['trial type (go or nogo)', \n",
    "                              'required response (left or right)', \n",
    "                              'when the stop signal is shown (or 0 if not)', \n",
    "                              'response time 1', \n",
    "                              'status 1 (1=correct, 2=wrong, 3=timeout)',\n",
    "                              'response time 2 (only in no go trials)',\n",
    "                              'status 2 (only in no go trials; 1=correct, 2=wrong, 3=timeout)',\n",
    "                              '1=trial is correct ; 0=trial is not correct']} \n",
    "\n",
    "column_meanings = pd.DataFrame(column_meanings)\n",
    "column_meanings.set_index('Column', inplace=True)\n",
    "column_meanings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_test(stop_df): # Jacob\n",
    "    \n",
    "    # renaming and reordering columns\n",
    "    stop_df.rename(columns = {0:'trial_type', 1:'correct_resp.', \n",
    "                            2:'stop_signal_delay', 3:'response_time',\n",
    "                            4:'status', 5:'resonse_time_nogo',\n",
    "                            6:'status_nogo', 7:'correct'}, inplace = True)\n",
    "\n",
    "    stop_df = stop_df[['participant', 'type', 'repeat', 'trial_type',\n",
    "                    'correct_resp.', 'correct', 'response_time',\n",
    "                    'status', 'stop_signal_delay', 'resonse_time_nogo',\n",
    "                    'status_nogo']]\n",
    "\n",
    "    # The average resonse time for go trials per trial type\n",
    "    avg_go_resp_time = stop_df[stop_df['trial_type'] == 'go'].groupby([\n",
    "        'participant', 'type','status']).mean()['response_time']\n",
    "\n",
    "\n",
    "    # The average resonse time for no-go trials per correct/incorrect trial\n",
    "    avg_nogo_resp_time = stop_df[stop_df['trial_type'] == 'nogo'].groupby([\n",
    "        'participant', 'type','status_nogo']).mean()['response_time']\n",
    "\n",
    "    # Good to keep in mind that here, status three corresponds with a correct trail\n",
    "    # Since there was no press in a no-go trial.\n",
    "\n",
    "    # Number of errors and time-outs in go trials\n",
    "    errors_timeout_go = stop_df[(stop_df['trial_type'] == 'go') & \n",
    "                                (stop_df['status'] != 1.0)].groupby([\n",
    "                                    'participant', 'type', 'repeat','status']).count()['trial_type']\n",
    "\n",
    "    # Number of errors and time-outs in no-go trials\n",
    "    errors_timeout_nogo = stop_df[stop_df['trial_type'] == 'nogo'].groupby([\n",
    "        'participant', 'type', 'repeat','status_nogo']).count()['trial_type']\n",
    "    \n",
    "    return avg_go_resp_time, avg_nogo_resp_time, errors_timeout_go, errors_timeout_nogo\n",
    "\n",
    "# callig the function\n",
    "avg_go_resp_time, avg_nogo_resp_time, errors_timeout_go, errors_timeout_nogo = stop_test(data_dict['stop'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "react_go_boxplot = data_dict['stop'][(data_dict['stop']['trial_type'] == 'go') & \n",
    "                                     (data_dict['stop']['correct'] == 1)][['response_time', 'participant', 'type']\n",
    "                                                      ].hvplot.box(by='type', \n",
    "                                                                   groupby='participant',\n",
    "                                                                   title='Reaction time for correct responses',\n",
    "                                                                   xlabel='Session Type', \n",
    "                                                                   ylabel='Resopnse Time (ms)')\n",
    "\n",
    "react_go_boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate percentage of errors/correct\n",
    "\n",
    "participants = ['blue', 'green', 'red', 'pink', 'orange']\n",
    "session_type = ['control', 'dehydratation']\n",
    "\n",
    "perc_correct = pd.DataFrame(index=[participants])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_dict['stop']\n",
    "correct = len(df[(df['participant'] == 'blue') &\n",
    "    (df['type'] == 'dehydration') &\n",
    "    (df['correct'] == 1)])\n",
    "total = len(df[(df['participant'] == 'blue') &\n",
    "    (df['type'] == 'dehydration')])\n",
    "\n",
    "perc_correct = (correct/total) * 100\n",
    "perc_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbal Fluency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbal_df = data_dict['verbal'].copy()\n",
    "verbal_df = verbal_df[verbal_df[1] != 'word count'] # to remove silly headers\n",
    "verbal_df.rename(columns={0:'word_type', 1:'n'}, inplace=True)\n",
    "verbal_df['n'] = verbal_df['n'].astype(int)\n",
    "\n",
    "verbal_avg = verbal_df.groupby(['participant', 'type']).mean().round(2)\n",
    "error_data = verbal_df.describe().transpose()\n",
    "\n",
    "verbal_avg_bar = verbal_avg.hvplot.bar(title='Average number of words produced per session type',\n",
    "                                        xlabel='Participant, Session Type', \n",
    "                                        ylabel ='Number of words').opts(xrotation=25)# * error_data.hvplot.errorbars(y='max', yerr1='std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def verbal_test(verbal_df): # Jacob\n",
    "    verbal_df = data_dict['verbal'].copy()\n",
    "    verbal_df = verbal_df[verbal_df[1] != 'word count'] # to remove silly headers\n",
    "    verbal_df.rename(columns={0:'word_type', 1:'n'}, inplace=True)\n",
    "    verbal_df['n'] = verbal_df['n'].astype(int)\n",
    "\n",
    "    verbal_avg = verbal_df.groupby(['participant', 'type']).mean().round(2)\n",
    "    \n",
    "    verbal_avg_bar = verbal_avg.hvplot.bar(title='Average number of words produced per session type',\n",
    "                                           xlabel='Participant, Session Type', \n",
    "                                           ylabel ='Number of words').opts(xrotation=25)# * verbal_avg.hvplot.errorbars(x=)\n",
    "\n",
    "    return verbal_df, verbal_avg, verbal_avg_bar\n",
    "\n",
    "verbal_df, verbal_avg, verbal_avg_bar = verbal_test(data_dict['verbal'])\n",
    "verbal_avg_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Span Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "\n",
    "def digit_test(digit_df): # Karina\n",
    "    data_types = {'participant': 'string',\n",
    "                'type': 'string',\n",
    "                'repeat': 'int',\n",
    "                'seq length':'float',\n",
    "                'errors': 'float',\n",
    "                'clicks expected': 'float',\n",
    "                'clicks observed': 'float'\n",
    "    }\n",
    "    digit_df = digit_df.astype(data_types)\n",
    "    digit_span_grouped = digit_df.groupby(['participant','type', 'repeat'])\n",
    "    digit_span_grouped = digit_span_grouped.agg({'seq length': 'max','errors': 'sum', 'clicks expected':'max','clicks observed':'max'} )\n",
    "\n",
    "    digit_span_grouped['clicks ratio'] = digit_span_grouped['clicks observed'] - digit_span_grouped['clicks expected']\n",
    "    digit_span_grouped['seq length'] = digit_span_grouped['seq length'] - 1\n",
    "    digit_span_grouped\n",
    "\n",
    "    digit_span_mean_sem = digit_span_grouped.groupby(['participant', 'type']).agg(['mean','sem'])\n",
    "\n",
    "    seq_length = digit_span_mean_sem['seq length']['mean'].tolist()\n",
    "    errors = digit_span_mean_sem['errors']['mean'].tolist() \n",
    "    participants = digit_span_mean_sem.reset_index().participant.unique().tolist()\n",
    "    sessions_type = digit_span_mean_sem.reset_index().type.unique().tolist()\n",
    "\n",
    "    return seq_length, errors, participants, sessions_type\n",
    "\n",
    "seq_length, errors, participants, sessions_type = digit_test(digit_df = df_dict[\"digit\"])\n",
    "\n",
    "\n",
    "def digit_barplots(participants,sessions_type, values, palette, y_label):\n",
    "\n",
    "    x = [ (participant, session) for participant in participants for session in sessions_type]\n",
    "    #[('blue', 'Control'), ('blue', 'Dehydration'), ('green', 'Control'), ('green', 'Dehydration'), ('orange', 'Control'), ('orange', 'Dehydration'), ('pink', 'Control'), ('pink', 'Dehydration'), ('red', 'Control'), ('red', 'Dehydration')]\n",
    "\n",
    "    source = ColumnDataSource(data=dict(x=x, counts=values))\n",
    "\n",
    "    p = figure(x_range=FactorRange(*x), height=350, title=\"Digit Span\",\n",
    "            toolbar_location=None, tools=\"\")\n",
    "\n",
    "    p.vbar(x='x', top='counts', width=0.9, source=source, line_color=\"white\",\n",
    "        fill_color=factor_cmap('x', palette=palette, factors=sessions_type, start=1, end=2))\n",
    "\n",
    "    p.y_range.start = 0\n",
    "    p.x_range.range_padding = 0.1\n",
    "    p.xaxis.major_label_orientation = 1\n",
    "    p.xgrid.grid_line_color = None\n",
    "\n",
    "    #  y-axis\n",
    "    p.yaxis.axis_label = y_label\n",
    "    p.yaxis.major_label_orientation = \"vertical\"\n",
    "\n",
    "    # x-axis\n",
    "    p.xaxis.axis_label = \"Participant\"\n",
    "\n",
    "    return(p)\n",
    "\n",
    "p_seq_lenght = digit_barplots(participants, sessions_type, values=seq_length, palette=['blue', 'grey'], y_label=\"Longest sequence remembered\")\n",
    "p_errors = digit_barplots(participants, sessions_type, values=errors, palette=['orange', 'grey'], y_label='Number of errors made')\n",
    "\n",
    "\n",
    "show(p_seq_lenght)\n",
    "show(p_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
