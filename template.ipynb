{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HydroHomies Plots\n",
    "In this notebook, the plots, figures and also some explanations or details about each of them are being presented.  \n",
    "\n",
    "To clarify plots, please follow this order:\n",
    "- Title for each plot is mandatory\n",
    "- Analysis must be written \n",
    "- legends are manedatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as stream:\n",
    "    config = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "code1"
    ]
   },
   "source": [
    "### Cleaning (Digit Span Raw Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_digit_span(raw_df):\n",
    "    # Select the sequence length data from the raw data and create a dataframe\n",
    "    seq_length_df = raw_df[raw_df[1].astype(str).str.match(r'\\d+')]\n",
    "    seq_length_df\n",
    "\n",
    "    # Get the value of the longest sequence remebered\n",
    "    longest = seq_length_df[2]\n",
    "    longest = longest.tolist()\n",
    "\n",
    "    # Get the number of errors made\n",
    "    error_number = seq_length_df[3]\n",
    "    error_number = error_number.tolist()\n",
    "\n",
    "    click_stim_df = raw_df[raw_df[1]=='clickedStim']\n",
    "    click_stim_df.size\n",
    "\n",
    "    clicks_observed = click_stim_df.count(axis=1) - 2 \n",
    "    clicks_observed = clicks_observed.tolist()\n",
    "\n",
    "    clicks_expected =  pd.to_numeric(longest) + 1\n",
    "    clicks_expected = clicks_expected.tolist()\n",
    "\n",
    "    clean_data = pd.DataFrame(data ={'seq length':longest,\n",
    "                        'errors': error_number,\n",
    "                        'clicks expected': clicks_expected,\n",
    "                        'clicks observed':clicks_observed})\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Integration For Each Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_df(config_dict):\n",
    "    data_dict = {}\n",
    "    \n",
    "    # select all files \n",
    "    files = {name: file for name, file in config_dict.items()} # files = config_dic.copy()\n",
    "    # read the files \n",
    "    for test, file in files.items():\n",
    "        df_dict = pd.read_excel(file, sheet_name=None, header=None)\n",
    "\n",
    "        for session, df in df_dict.items():\n",
    "            # extracting the participant name and type name\n",
    "            participant = test.split('_')[-1]\n",
    "            test_name = test.split('_')[0]\n",
    "            #extracting repeat number and making its column except for personal\n",
    "            try:\n",
    "                type, repeat = session.split('_')\n",
    "                df.insert(0, 'repeat', repeat)\n",
    "            except ValueError:\n",
    "                type = session\n",
    "\n",
    "            # inserting the type and participant columns\n",
    "            df.insert(0, 'type', type)\n",
    "            df.insert(0, 'participant', participant)\n",
    "\n",
    "            #cleaning digit_span row data\n",
    "            if test_name == 'digit_span':\n",
    "                try:\n",
    "                    df = clean_digit_span(df.iloc[3:])\n",
    "                except:\n",
    "                    continue   \n",
    "            # concatenating data frames of each test\n",
    "            if test_name not in data_dict:\n",
    "                data_dict[test_name] = df\n",
    "            else:\n",
    "                data_dict[test_name] = pd.concat([data_dict[test_name], df])\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "data_dict = create_merged_df(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flanker Test Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# matplotlib.style.use('ggplot')\n",
    "\n",
    "def flanker_test(flanker_df): # Roya\n",
    "    flanker_df.rename(columns={0:\"pattern\", 1:\"expression\", 2:\"correctness\", 3:\"response-time\"}, inplace=True)\n",
    "    flanker_df[\"correctness\"] = flanker_df[\"correctness\"].replace(1, \"correct\")\n",
    "    flanker_df[\"correctness\"] = flanker_df[\"correctness\"].replace(2, \"incorrect\")\n",
    "    flanker_df[\"correctness\"] = flanker_df[\"correctness\"].replace(3, \"not-answer\")\n",
    "    flanker_df = flanker_df[flanker_df[\"correctness\"] == \"correct\"]\n",
    "    \n",
    "    df = flanker_df.groupby([\"participant\", \"type\"])[\"correctness\"].value_counts()\n",
    "\n",
    "\n",
    "    dehydration = flanker_df[flanker_df[\"type\"] == \"dehydration\"]\n",
    "    control = flanker_df[flanker_df[\"type\"] == \"control\"]\n",
    "    dehydration = dehydration.groupby([\"participant\"])[\"correctness\"].count().rename(\"dehydration\").to_frame()\n",
    "    control = control.groupby([\"participant\"])[\"correctness\"].count().rename(\"control\").to_frame()\n",
    "    df = dehydration.join(control)\n",
    "    df.plot(kind=\"bar\", width=0.3, title=\" correct answers\")\n",
    "\n",
    "data_dict = create_merged_df(config)\n",
    "# print(data_dict[\"flanker\"])\n",
    "# data_dict[\"flanker\"].to_csv(\"data.csv\")\n",
    "flanker_test(data_dict[\"flanker\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stroop Test  Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stroop_test(stroop_df): # Mahdiye\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Signal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_meanings = {'Column':[0,1,2,3,4,5,6,7],\n",
    "                   'Meaning':['trial type (go or nogo)', \n",
    "                              'required response (left or right)', \n",
    "                              'when the stop signal is shown (or 0 if not)', \n",
    "                              'response time 1', \n",
    "                              'status 1 (1=correct, 2=wrong, 3=timeout)',\n",
    "                              'response time 2 (only in no go trials)',\n",
    "                              'status 2 (only in no go trials; 1=correct, 2=wrong, 3=timeout)',\n",
    "                              '1=trial is correct ; 0=trial is not correct']} \n",
    "\n",
    "column_meanings = pd.DataFrame(column_meanings)\n",
    "column_meanings.set_index('Column', inplace=True)\n",
    "column_meanings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_test(stop_df): # Jacob\n",
    "    \n",
    "    # renaming and reordering columns\n",
    "    stop_df.rename(columns = {0:'trial_type', 1:'correct_resp.', \n",
    "                            2:'stop_signal_delay', 3:'response_time',\n",
    "                            4:'status', 5:'resonse_time_nogo',\n",
    "                            6:'status_nogo', 7:'correct'}, inplace = True)\n",
    "\n",
    "    stop_df = stop_df[['participant', 'type', 'repeat', 'trial_type',\n",
    "                    'correct_resp.', 'correct', 'response_time',\n",
    "                    'status', 'stop_signal_delay', 'resonse_time_nogo',\n",
    "                    'status_nogo']]\n",
    "\n",
    "    # The average resonse time for go trials per trial type\n",
    "    avg_go_resp_time = stop_df[stop_df['trial_type'] == 'go'].groupby([\n",
    "        'participant', 'type','status']).mean()['response_time']\n",
    "\n",
    "\n",
    "    # The average resonse time for no-go trials per correct/incorrect trial\n",
    "    avg_nogo_resp_time = stop_df[stop_df['trial_type'] == 'nogo'].groupby([\n",
    "        'participant', 'type','status_nogo']).mean()['response_time']\n",
    "\n",
    "    # Good to keep in mind that here, status three corresponds with a correct trail\n",
    "    # Since there was no press in a no-go trial.\n",
    "\n",
    "    # Number of errors and time-outs in go trials\n",
    "    errors_timeout_go = stop_df[(stop_df['trial_type'] == 'go') & \n",
    "                                (stop_df['status'] != 1.0)].groupby([\n",
    "                                    'participant', 'type', 'repeat','status']).count()['trial_type']\n",
    "\n",
    "    # Number of errors and time-outs in no-go trials\n",
    "    errors_timeout_nogo = stop_df[stop_df['trial_type'] == 'nogo'].groupby([\n",
    "        'participant', 'type', 'repeat','status_nogo']).count()['trial_type']\n",
    "    \n",
    "    return avg_go_resp_time, avg_nogo_resp_time, errors_timeout_go, errors_timeout_nogo\n",
    "\n",
    "# callig the function\n",
    "avg_go_resp_time, avg_nogo_resp_time, errors_timeout_go, errors_timeout_nogo = stop_test(data_dict['stop'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "react_go_boxplot = data_dict['stop'][(data_dict['stop']['trial_type'] == 'go') & \n",
    "                                     (data_dict['stop']['correct'] == 1)][['response_time', 'participant', 'type']\n",
    "                                                      ].hvplot.box(by='type', \n",
    "                                                                   groupby='participant',\n",
    "                                                                   title='Reaction time for correct responses',\n",
    "                                                                   xlabel='Session Type', \n",
    "                                                                   ylabel='Resopnse Time (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of correct answers.\n",
    "\n",
    "participants = ['blue', 'green', 'red', 'pink', 'orange']\n",
    "df = data_dict['stop']\n",
    "perc_correct = {part:0 for part in participants}\n",
    "df[df['participant'] == participants[0]][['type', 'correct']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbal Fluency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def verbal_test(verbal_df): # Jacob\n",
    "    verbal_df = data_dict['verbal'].copy()\n",
    "    verbal_df = verbal_df[verbal_df[1] != 'word count'] # to remove silly headers\n",
    "    verbal_df.rename(columns={0:'word_type', 1:'n'}, inplace=True)\n",
    "    verbal_df['n'] = verbal_df['n'].astype(int)\n",
    "\n",
    "    verbal_avg = verbal_df.groupby(['participant', 'type']).mean().round(2)\n",
    "    \n",
    "    verbal_avg_bar = verbal_avg.hvplot.bar(title='Average number of words produced per session type',\n",
    "                                           xlabel='Participant, Session Type', \n",
    "                                           ylabel ='Number of words').opts(xrotation=25) * verbal_avg.hvplot.errorbars(x=)\n",
    "\n",
    "    return verbal_df, verbal_avg, verbal_avg_bar\n",
    "\n",
    "verbal_df, verbal_avg, verbal_avg_bar = verbal_test(data_dict['verbal'])\n",
    "verbal_df.head()\n",
    "# verbal_avg_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Span Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_test(digit_df): # Karina\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:23:14) [GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b91fa5cffef32cb10787a50fea9666676a7951181e01280b4644cd70c964bf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
